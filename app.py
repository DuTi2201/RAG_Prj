import streamlit as st
import torch
import tempfile
import os
from dotenv import load_dotenv
from transformers import (
    BitsAndBytesConfig,
    AutoTokenizer,
    AutoModelForCausalLM,
    pipeline
)
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline
from langchain_community.document_loaders import PyPDFLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_chroma import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain import hub

# Load environment variables
load_dotenv()

# Configuration
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME", "bkai-foundation-models/vietnamese-bi-encoder")
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "lmsys/vicuna-7b-v1.5")
MAX_NEW_TOKENS = int(os.getenv("MAX_NEW_TOKENS", "512"))

# Streamlit page configuration
st.set_page_config(
    page_title="RAG - H·ªèi ƒë√°p t√†i li·ªáu AIO",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize session state
if "rag_chain" not in st.session_state:
    st.session_state.rag_chain = None
if "models_loaded" not in st.session_state:
    st.session_state.models_loaded = False
if "embeddings" not in st.session_state:
    st.session_state.embeddings = None
if "llm" not in st.session_state:
    st.session_state.llm = None
if "pdf_processed" not in st.session_state:
    st.session_state.pdf_processed = False

@st.cache_resource
def load_embeddings():
    """Load and cache the embedding model"""
    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        return embeddings
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i embedding model: {str(e)}")
        return None

@st.cache_resource
def load_llm():
    """Load and cache the LLM model"""
    try:
        # Configure quantization for memory efficiency
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        # Load tokenizer
        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Load model
        model = AutoModelForCausalLM.from_pretrained(
            LLM_MODEL_NAME,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # Create pipeline
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=MAX_NEW_TOKENS,
            pad_token_id=tokenizer.eos_token_id,
            device_map="auto",
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
        
        # Integrate with LangChain
        llm = HuggingFacePipeline(pipeline=text_generation_pipeline)
        return llm
    except Exception as e:
        st.error(f"L·ªói khi t·∫£i LLM model: {str(e)}")
        return None

def format_docs(docs):
    """Format retrieved documents for context"""
    return "\n\n".join(doc.page_content for doc in docs)

def process_pdf(uploaded_file, embeddings, llm):
    """Process uploaded PDF and create RAG chain"""
    try:
        # Save uploaded file to temporary location
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # Load PDF
        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        
        # Initialize semantic chunker
        semantic_splitter = SemanticChunker(
            embeddings=embeddings,
            buffer_size=int(os.getenv("SEMANTIC_BUFFER_SIZE", "1")),
            breakpoint_threshold_type=os.getenv("SEMANTIC_BREAKPOINT_THRESHOLD_TYPE", "percentile"),
            breakpoint_threshold_amount=int(os.getenv("SEMANTIC_BREAKPOINT_THRESHOLD_AMOUNT", "95")),
            min_chunk_size=int(os.getenv("MIN_CHUNK_SIZE", "500")),
            add_start_index=True
        )
        
        # Split documents into chunks
        docs = semantic_splitter.split_documents(documents)
        
        # Create vector database
        vectorstore = Chroma.from_documents(
            documents=docs,
            embedding=embeddings
        )
        
        # Create retriever
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}
        )
        
        # Get RAG prompt
        prompt = hub.pull("rlm/rag-prompt")
        
        # Create RAG chain
        rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )
        
        # Clean up temporary file
        os.unlink(tmp_file_path)
        
        return rag_chain, len(docs)
        
    except Exception as e:
        # Clean up temporary file in case of error
        if 'tmp_file_path' in locals():
            try:
                os.unlink(tmp_file_path)
            except:
                pass
        raise e

# Main UI
st.title("üìö RAG - H·ªèi ƒë√°p t√†i li·ªáu b√†i h·ªçc AIO")
st.markdown("""
### H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:
1. **T·∫£i models**: ·ª®ng d·ª•ng s·∫Ω t·ª± ƒë·ªông t·∫£i c√°c models c·∫ßn thi·∫øt khi kh·ªüi ch·∫°y l·∫ßn ƒë·∫ßu
2. **Upload PDF**: Ch·ªçn file PDF t√†i li·ªáu b√†i h·ªçc c·∫ßn h·ªèi ƒë√°p
3. **X·ª≠ l√Ω PDF**: Nh·∫•n n√∫t "X·ª≠ l√Ω PDF" ƒë·ªÉ ph√¢n t√≠ch v√† t·∫°o vector database
4. **ƒê·∫∑t c√¢u h·ªèi**: Nh·∫≠p c√¢u h·ªèi v·ªÅ n·ªôi dung t√†i li·ªáu v√† nh·∫≠n c√¢u tr·∫£ l·ªùi

---
""")

# Load models section
if not st.session_state.models_loaded:
    st.info("üîÑ ƒêang t·∫£i models... Vui l√≤ng ƒë·ª£i trong gi√¢y l√°t.")
    
    # Load embedding model
    with st.spinner("ƒêang t·∫£i Embedding Model..."):
        st.session_state.embeddings = load_embeddings()
    
    if st.session_state.embeddings is not None:
        st.success("‚úÖ Embedding Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
        
        # Load LLM
        with st.spinner("ƒêang t·∫£i Large Language Model (Vicuna 7B)... ƒê√¢y l√† b∆∞·ªõc t·ªën th·ªùi gian nh·∫•t."):
            st.session_state.llm = load_llm()
        
        if st.session_state.llm is not None:
            st.success("‚úÖ Large Language Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
            st.session_state.models_loaded = True
            st.rerun()
        else:
            st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i Large Language Model. Vui l√≤ng th·ª≠ l·∫°i.")
            st.stop()
    else:
        st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i Embedding Model. Vui l√≤ng th·ª≠ l·∫°i.")
        st.stop()
else:
    st.success("‚úÖ T·∫•t c·∫£ models ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
    
    # File upload section
    st.subheader("üìÑ Upload v√† x·ª≠ l√Ω PDF")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file PDF t√†i li·ªáu b√†i h·ªçc:",
        type="pdf",
        help="Ch·ªçn file PDF ch·ª©a n·ªôi dung b√†i h·ªçc c·∫ßn h·ªèi ƒë√°p"
    )
    
    if uploaded_file is not None:
        st.info(f"üìÅ ƒê√£ ch·ªçn file: {uploaded_file.name}")
        
        if st.button("üîÑ X·ª≠ l√Ω PDF", type="primary"):
            with st.spinner("ƒêang x·ª≠ l√Ω PDF v√† t·∫°o vector database..."):
                try:
                    rag_chain, num_chunks = process_pdf(
                        uploaded_file, 
                        st.session_state.embeddings, 
                        st.session_state.llm
                    )
                    st.session_state.rag_chain = rag_chain
                    st.session_state.pdf_processed = True
                    st.success(f"‚úÖ X·ª≠ l√Ω PDF th√†nh c√¥ng! ƒê√£ t·∫°o {num_chunks} chunks.")
                except Exception as e:
                    st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω PDF: {str(e)}")
    
    # Q&A section
    if st.session_state.pdf_processed and st.session_state.rag_chain is not None:
        st.subheader("üí¨ H·ªèi ƒë√°p v·ªõi t√†i li·ªáu")
        
        question = st.text_input(
            "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:",
            placeholder="V√≠ d·ª•: N·ªôi dung ch√≠nh c·ªßa b√†i h·ªçc n√†y l√† g√¨?",
            help="ƒê·∫∑t c√¢u h·ªèi li√™n quan ƒë·∫øn n·ªôi dung trong file PDF ƒë√£ upload"
        )
        
        if question:
            with st.spinner("ƒêang tr·∫£ l·ªùi..."):
                try:
                    response = st.session_state.rag_chain.invoke(question)
                    
                    # Clean up response (remove "Answer:" prefix if present)
                    if response.startswith("Answer:"):
                        response = response[7:].strip()
                    
                    st.subheader("üìù C√¢u tr·∫£ l·ªùi:")
                    st.write(response)
                    
                except Exception as e:
                    st.error(f"‚ùå L·ªói khi tr·∫£ l·ªùi c√¢u h·ªèi: {str(e)}")
    
    elif not st.session_state.pdf_processed:
        st.info("üìã Vui l√≤ng upload v√† x·ª≠ l√Ω file PDF tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.")

# Sidebar with additional information
with st.sidebar:
    st.header("‚ÑπÔ∏è Th√¥ng tin d·ª± √°n")
    st.markdown("""
    **C√¥ng ngh·ªá s·ª≠ d·ª•ng:**
    - ü§ñ LangChain Framework
    - üß† Vicuna 7B LLM
    - üîç Vietnamese Bi-Encoder
    - üìä ChromaDB Vector Store
    - üé® Streamlit Interface
    
    **T√≠nh nƒÉng:**
    - ‚úÇÔ∏è Semantic Chunking
    - üîç Vector Similarity Search
    - üíæ Model Caching
    - üöÄ 4-bit Quantization
    """)
    
    st.header("‚öôÔ∏è C·∫•u h√¨nh hi·ªán t·∫°i")
    st.code(f"""
    Embedding Model: {EMBEDDING_MODEL_NAME}
    LLM Model: {LLM_MODEL_NAME}
    Max New Tokens: {MAX_NEW_TOKENS}
    Models Loaded: {st.session_state.models_loaded}
    PDF Processed: {st.session_state.pdf_processed}
    """)